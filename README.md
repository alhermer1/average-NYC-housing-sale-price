# average-NYC-housing-sale-price


This was my first ever data science project I completed sophmore year of college. I have progressed a lot since. Will maybe one day revisit.



Write-up:

The first thing I did in this assignment was load up the data into Jupyter notebook, and display the dataframe. In the dataframe, there were 21 different variables, so I used the info function to find what type each variable was. Then I decided to drop certain columns. I dropped 'Unnamed: 0' because it was a structural column used in the original csv file. I dropped 'BUILDING CLASS CATEGORY' because it was a subset of ‘RESIDENTIAL’ and ‘COMMERCIAL’ Units columns. I dropped 'EASE-MENT' because it was completely empty. I dropped ‘ADDRESS’ because it is a subset of ‘BOROUGH’, and wildly complicated to recode. I dropped 'SALE DATE' because the date itself should not influence the price itself. I dropped ‘BUILDING CLASS AT PRESENT' and’ TAX CLASS AT PRESENT' because I intended on looking at variables that determined price, so their current states, after being sold, do not matter to me. I dropped’ APARTMENT NUMBER' because that is an arbitrary assignment that is unnormalized and has no importance across buildings. I dropped 'ZIP CODE' and NEIGHBORHOOD' because those are subsets of ‘BOROUGH’. And finally I dropped 'YEAR BUILT' primarily because I had a very difficult time recoding this variable. I wanted to recode by time bins (ie 1950-2000, 1900s-1950…), however my lack of python syntax knowledge made it impossible to overcome the associated debugging. So I just cut this out. After dropping the columns, I realized from my initial displayed data frame that there are missing values in the SALE PRICE column. If it were any other column, I would fill the missing values with the mean values of the column, however because we are trying to predict what determines “SALE PRICE” I thought it would be important to just outright delete the rows without the values. Knowing that there were still many thousands of columns remaining, I think the insertion of bias (at least a lot of bias) was avoided. Following this transformation of the data, I looked back at the list displaying the data types, and noticed that ‘SALE PRICE,’ ‘LAND SQUARE FEET,’ and ‘GROSS SQUARE FEET’  were all listed as objects, so I made sure to switch them to numerical variables. Then I noticed that ‘LAND SQUARE FEET,’ and ‘GROSS SQUARE FEET’  had Na values that occured as a result of the change to numeric, so I switched those values to the means of their respective columns. I also saw that 'BOROUGH,’ 'TAX CLASS AT TIME OF SALE,’ and 'BUILDING CLASS AT TIME OF SALE' were incorrectly classified, so I switched them to categorical type variables. Then I used the info function again to see that the variables had succesfully changed, which they did. After that, I wanted to see the frequency distribution of my three categorical variables. I created bar plots for each of the variables, and the ‘BOROUGH’ and 'TAX CLASS AT TIME OF SALE' charts were very straight forward and ready for hot-coding, however 'BUILDING CLASS AT TIME OF SALE' had a somewhat complex distribution. I realized, however that I could group the building classes into broader categories, according to their letter, so I hardcoded each available option to a parent letter building class. After that, I recreated a bar graph, and saw a much cleaner distribution of building classes across the properties. Now that I had cleaned the categorical variables, I decided to one-hot encode each of them into the original dataframe. After the encoding, I wanted to see the correlation between features. I started off by doing a numeric correlation matrix, followed by a categorical correlation list. There were so many correlation features, due to the one hot encoding, that I decided to add a line of code to only include features that had a correlation above 0.02 with SALE PRICE. From this list alone, I felt like I could hypothesize that GROSS square feet definitely played the largest part in determining the final score. Regardless, I then ran some regression models. In each of the regression models, they shared the following format: X stored some combination of independent variables, while Y stored the response variable ‘SALE PRICE.’ Then I split X and Y, using a test_size of 0.2 and a random_state of 42 into X and Y training and testing sets. I than created a linear regression model (be it that this is a regression type problem) and I fit it with the X and Y training data. Subsequently I printed the training and testing scores. I used this in each of my my 4 training models. In my first model, I used all of the cleaned data and ran it through the model. In the following three models, I used for loops and “best” value initializers to determine which columns led to the highest training and testing scores. For model 2, I was checking to see which 1 variable best predicts sales price. For model 3, I was checking which 2 variables best predict sales price. And for model 4, I was checking which 3 variables best predict sales price. I would have repeated this more more variable combinations, however due to only a basic knowledge of CS programming ideas, I do not know how to code what I am doing in a time efficient manner. 3 variables took a long time to run, and 4 variables would not run efficiently.

From the results I got, it seemed as if the model that just tested the cleaned data had the highest training score and testing, which would make the most sense. We can expect that more variables can better predict sales price, however the goal of the assignment was to see which variables best predicted sales price. From this, and our results, we saw that throughout each model, GROSS SQUARE FEET consistently was returned as the best predictor of final SALE PRICE. In models 3 and 4, the training scores and testing score seemed to be roughly the same, despite additional variables being credited for predicting sale price. This means that even with the addition of new variables, it seemed as if the best  predictor was GROSS SQUARE FEET, which was a part of my hypothesis. However, the scores we got in models 2-4 were much lower than the first general linear regression model, suggesting that which GROSS SQUARE FEET may best predict sales price, the model really is suggesting that more combinations of variables are required to accurately predict SALE PRICE. In addition, when looking at the first linear regression model’s training and testing scores, it seems as if these scores are also relatively small. I think what may have happened here, is that some of the variables I initially cleaned and dropped from the dataset in reality played a more important role in determining final price than I initially thought. With a deeper understanding of how to recode these values, and more python knowledge, I would aim in the future to incorporate the results and see how they impact the model.

